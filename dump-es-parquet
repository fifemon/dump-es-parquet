#!/usr/bin/env python
"""
Write parquet files from elasticsearch indices, using polars.
"""

from argparse import ArgumentParser
import logging
import requests
import traceback
import re
import json
from datetime import datetime

from opensearchpy import OpenSearch, RequestsHttpConnection
import polars as pl

# https://www.elastic.co/guide/en/elasticsearch/reference/6.8/mapping-types.html
# https://docs.pola.rs/py-polars/html/reference/datatypes.html
es2pl_type = {
    'integer': pl.Int32,
    'long': pl.Int64,
    'string': pl.String,
    'keyword': pl.String,
    'boolean': pl.Boolean,
    'date': pl.Datetime,
}

logger = logging.getLogger(__name__)

class Processor(object):
    def __init__(self, es, index, size=500, timeout=60, flatten=False):
        self.es = es
        self.index = index
        self.size = size
        self.timeout = timeout
        self.flatten = flatten

        self.es_client = OpenSearch(self.es, timeout=timeout, 
                                        connection_class = RequestsHttpConnection)

    def process(self):
        for idx in self.get_indices(self.index):
            try:
                logger.log(99,'Processing index %s'%idx)
                self.process_index(idx)
            except:
                logger.error('Exception while processing index %s'%idx)
                traceback.print_exc()
                continue

    def map_properties(self, m, prefix=''):
        """Map elasticsearch types to polars types"""
        r={}
        for field,typ in m['properties'].items():
            if 'properties' in typ:
                if self.flatten:
                    r.update(self.map_properties(typ,prefix=field+'_'))
                else:
                    r[prefix+field] = pl.Struct(self.map_properties(typ))
            else:
                r[prefix+field] = es2pl_type.get(typ['type'],None)
        return r
    
    def map_source(self, schema, src, prefix=''):
        """Map elasticsearch source doc to native types"""
        dest = {}
        for k, v in src.items():
            pk = prefix+k
            if type(v) is dict:
                dest.update(self.map_source(schema, v, prefix=k+'_'))
            elif pk not in schema:
                logger.warning(f'field {pk} not in schema; dropping')
            elif schema[pk] == pl.Datetime:
                try:
                    dest[pk] = datetime.fromisoformat(v)
                except ValueError:
                    try:
                        i = int(v)
                        if i < 20000000000:
                            # guess if the epoch number is seconds or milliseconds
                            # if seconds this is Tue Oct 11 07:33:20 EDT 2603
                            # if it's actually millis: Thu Aug 20 07:33:20 EDT 1970
                            # so as long as we don't need to deal with any dates in 1970 
                            # or after the singularity this is fine... even then,
                            # if the date actually is in 1970, the year will at least be correct!
                            dest[pk] = datetime.fromtimestamp(i)
                        else:
                            dest[pk] = datetime.fromtimestamp(i/1000)
                    except ValueError:
                        logger.warning(f'unable to convert field {pk} with value {v} to datetime')
            else:
                dest[pk] = v
        return dest

    def index_schema(self,index):
        mapping = self.es_client.indices.get_mapping(index=index)
        logger.debug(mapping)
        for idx,dts in mapping.items():
            for dt,m in dts['mappings'].items():
                return self.map_properties(m)

    def process_index(self,index):
        schema = self.index_schema(index)
        logger.debug(schema)
        s = self.es_client.search(index=index,
                                    scroll="5m",
                                    size=self.size)
        all_hits = s['hits']['total']
        if all_hits == 0:
            logger.error('No records found')
            return
        #print(s['hits']['hits'][0])
        logger.log(99,'Processing %d records'%all_hits)
        df = pl.DataFrame([self.map_source(schema, d['_source']) for d in s['hits']['hits']], schema=schema, orient='rows')
        sent_hits = len(s['hits']['hits'])
        logger.info('read %d/%d records'%(sent_hits,all_hits))
        #print(df)
        #return
        while True:
            s = self.es_client.scroll(scroll_id=s['_scroll_id'],scroll="5m")
            if len(s['hits']['hits']) == 0:
                break
            df2 = pl.DataFrame([self.map_source(schema, d['_source']) for d in s['hits']['hits']], schema=schema, orient='rows')
            df = df.vstack(df2)
            sent_hits += len(s['hits']['hits'])
            logger.info('read %d/%d records'%(sent_hits,all_hits))
        fname = index+'.parquet'
        df.write_parquet(fname)
        logger.log(99,f'Processing complete. {sent_hits}/{all_hits} records written to {fname}')

    def get_indices(self, index_pattern):
        idxs = self.es_client.indices.get_settings(index=index_pattern)
        return idxs.keys()


if __name__=="__main__":
    parser = ArgumentParser(description='Dump documents to parquet files')
    parser.add_argument('index',help='source index pattern')
    parser.add_argument('--es',help='source cluster address',default="http://localhost:9200")
    # we can just use the index name for each file
    #TODO: hive partitioning? https://duckdb.org/docs/data/partitioning/hive_partitioning
    #parser.add_argument('--filename',help='parquet file name',default="elastic.parquet")
    parser.add_argument('--size',help='Record batch size (default 500)',default=500,type=int)
    parser.add_argument('--timeout',help='Elasticsearch read timeout in seconds (default 60)',default=60,type=int)
    parser.add_argument('--flatten',help='Flatten nested data into top level, otherwise use structs',action='store_true')
    parser.add_argument('--debug',help='Enable debug logging',action='store_true')
    parser.add_argument('--quiet',help='Disable most logging (ignored if --debug specified)',action='store_true')

    args = parser.parse_args()

    logfmt = "%(asctime)s [%(levelname)s %(name)s] %(message)s"
    if args.debug:
        logging.basicConfig(level=logging.DEBUG, format=logfmt)
    elif args.quiet:
        logging.basicConfig(level=logging.ERROR, format=logfmt)
    else:
        logging.basicConfig(level=logging.INFO, format=logfmt)
    logging.addLevelName(99,'UBER')

    processor = Processor(args.es, args.index, args.size, args.timeout, args.flatten)
    processor.process()
