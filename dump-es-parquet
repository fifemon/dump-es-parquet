#!/usr/bin/env python
"""
Write parquet files from elasticsearch indices, using polars.
"""

from argparse import ArgumentParser
import logging
import requests
import traceback
import re
import json
from datetime import datetime
from collections import defaultdict

from opensearchpy import OpenSearch, RequestsHttpConnection
import polars as pl

# https://www.elastic.co/guide/en/elasticsearch/reference/6.8/mapping-types.html
# https://docs.pola.rs/py-polars/html/reference/datatypes.html
es2pl_type = {
    'byte': pl.Int8,
    'short': pl.Int16,
    'integer': pl.Int32,
    'long': pl.Int64,
    'half_float': pl.Float32,
    'float': pl.Float32,
    'double': pl.Float64,
    'string': pl.String,
    'text': pl.String,
    'keyword': pl.String,
    'boolean': pl.Boolean,
    'date': pl.Datetime,
}

logger = logging.getLogger(__name__)

class Processor(object):
    def __init__(self, es, index, size=500, timeout=60, flatten=False, query=None, partition_size=1000):
        self.es = es
        self.index = index
        self.size = size
        self.timeout = timeout
        self.flatten = flatten
        self.query = query
        self.partition_size = partition_size

        self.es_client = OpenSearch(self.es, timeout=timeout, 
                                        connection_class = RequestsHttpConnection)

        self.doc_warnings = defaultdict(lambda: 0)

    def process(self):
        for idx in self.get_indices(self.index):
            try:
                logger.log(99,'Processing index %s'%idx)
                self.process_index(idx)
            except:
                logger.error('Exception while processing index %s'%idx)
                traceback.print_exc()
                continue

    def map_properties(self, m, prefix=''):
        """Map elasticsearch types to polars types"""
        r={}
        for field,typ in m['properties'].items():
            if 'properties' in typ:
                if self.flatten:
                    r.update(self.map_properties(typ,prefix=field+'_'))
                else:
                    r[prefix+field] = pl.Struct(self.map_properties(typ))
            else:
                r[prefix+field] = es2pl_type.get(typ['type'],None)
        return r
    
    def map_source(self, schema, src, prefix=''):
        """Map elasticsearch source doc to native types"""
        dest = {}
        for k, v in src.items():
            pk = prefix+k
            if pk not in schema:
                self.log_warning(f'field {pk} not in schema; dropping')
                continue
                # we really shouldn't be modifying the schema here. Also, we can't stack
                # data frames of different widths, so after the first batch the schema is effectively fixed.
                #try:
                #    dest[pk] = str(v)
                #    schema[pk] = pl.String
                #    self.log_warning(f'field {pk} not in schema, setting to string')
                #except ValueError:
                #    self.log_warning(f'field {pk} not in schema, and can\'t convert to string; dropping')

            # elasticsearch can handle multiple values for a field, without changing the index mapping
            # (it's just the scalar type), while we'd have set the dataframe schema to a list in advance.
            # Taking the first value is an imperfect compromise.
            if type(v) is list:
                self.log_warning(f'field {pk} is list - keeping first value')
                v = v[0]

            if schema[pk] == pl.Struct:
                if self.flatten:
                    dest.update(self.map_source(schema, v, prefix=k+'_'))
                else:
                    dest[pk] = self.map_source(schema[pk].to_schema(), v)
                continue
            elif schema[pk] == pl.Datetime:
                try:
                    dest[pk] = datetime.fromisoformat(v)
                except ValueError:
                    try:
                        i = int(v)
                        if i < 20000000000:
                            # guess if the epoch number is seconds or milliseconds
                            # if seconds this is Tue Oct 11 07:33:20 EDT 2603
                            # if it's actually millis: Thu Aug 20 07:33:20 EDT 1970
                            # so as long as we don't need to deal with any dates in 1970 
                            # or after the singularity this is fine... even then,
                            # if the date actually is in 1970, the year will at least be correct!
                            dest[pk] = datetime.fromtimestamp(i)
                        else:
                            dest[pk] = datetime.fromtimestamp(i/1000)
                    except ValueError:
                        self.log_warning(f'unable to convert field {pk} to datetime')
            elif schema[pk] in (pl.Int8, pl.Int16, pl.Int32, pl.Int64):
                try:
                    dest[pk] = int(v)
                except ValueError:
                    try:
                        dest[pk] = int(float(v))
                    except ValueError:
                        self.log_warning(f'unable to convert field {pk} to int')
            elif schema[pk] in (pl.Float32, pl.Float64):
                try:
                    dest[pk] = float(v)
                except ValueError:
                    self.log_warning(f'unable to convert field {pk} to float')
            elif schema[pk] == pl.String:
                try:
                    dest[pk] = str(v)
                except ValueError:
                    self.log_warning(f'unable to convert field {pk} to string')
            else:
                dest[pk] = v
        return dest

    def index_schema(self,index):
        mapping = self.es_client.indices.get_mapping(index=index)
        logger.debug(mapping)
        for idx,dts in mapping.items():
            for dt,m in dts['mappings'].items():
                return self.map_properties(m)

    def process_index(self,index):
        schema = self.index_schema(index)
        logger.debug(schema)
        s = self.es_client.search(index=index,
                                  q=self.query,
                                  scroll="5m",
                                  size=self.size)
        all_hits = s['hits']['total']
        if all_hits == 0:
            logger.error('No records found')
            return

        logger.log(99,'Processing %d records'%all_hits)
        df = pl.DataFrame([self.map_source(schema, d['_source']) for d in s['hits']['hits']], schema=schema, orient='rows')
        sent_hits = len(s['hits']['hits'])
        logger.info('read %d/%d records'%(sent_hits,all_hits))

        partition = 0
        while True:
            s = self.es_client.scroll(scroll_id=s['_scroll_id'],scroll="5m")
            if len(s['hits']['hits']) == 0:
                break
            df2 = pl.DataFrame([self.map_source(schema, d['_source']) for d in s['hits']['hits']], schema=schema, orient='rows')
            if df is None:
                df = df2
            else:
                df = df.vstack(df2)
            sent_hits += len(s['hits']['hits'])
            logger.info('read %d/%d records'%(sent_hits,all_hits))

            if df.estimated_size('mb') > 1000:
                fname = index+f'-{partition:04}.parquet'
                df.write_parquet(fname)
                logger.log(99,f'Partition size reached. {sent_hits}/{all_hits} records written to {fname}')
                partition += 1
                df = None
                sent_hits = 0

        for msg,cnt in self.doc_warnings.items():
            logger.warning(f'{msg} [{cnt} documents]')

        if partition == 0:
            fname = index+'.parquet'
        else:
            fname = index+f'-{partition:04}.parquet'
        df.write_parquet(fname)
        logger.log(99,f'Processing complete. {sent_hits}/{all_hits} records written to {fname}.')

    def get_indices(self, index_pattern):
        idxs = self.es_client.indices.get_settings(index=index_pattern)
        return idxs.keys()

    def log_warning(self, msg):
        self.doc_warnings[msg] += 1


if __name__=="__main__":
    parser = ArgumentParser(description='Dump documents to parquet files')
    parser.add_argument('index',help='source index pattern')
    parser.add_argument('--es',help='source cluster address',default="http://localhost:9200")
    # we can just use the index name for each file
    #TODO: hive partitioning? https://duckdb.org/docs/data/partitioning/hive_partitioning
    #parser.add_argument('--filename',help='parquet file name',default="elastic.parquet")
    parser.add_argument('--size',help='Record batch size (default 500)',default=500,type=int)
    parser.add_argument('--timeout',help='Elasticsearch read timeout in seconds (default 60)',default=60,type=int)
    parser.add_argument('--flatten',help='Flatten nested data into top level, otherwise use structs',action='store_true')
    parser.add_argument('--query',help='Query string to filter results',default=None,type=str)
    parser.add_argument('--max-partition-size-mb',help='Maximum in-memory size of partition dataframe in megabytes (default 1000). '
                        'Note that the file size will be smaller due to compression',
                        default=1000,type=int)
    parser.add_argument('--debug',help='Enable debug logging',action='store_true')
    parser.add_argument('--quiet',help='Disable most logging (ignored if --debug specified)',action='store_true')

    args = parser.parse_args()

    logfmt = "%(asctime)s [%(levelname)s %(name)s] %(message)s"
    if args.debug:
        logging.basicConfig(level=logging.DEBUG, format=logfmt)
    elif args.quiet:
        logging.basicConfig(level=logging.ERROR, format=logfmt)
    else:
        logging.basicConfig(level=logging.INFO, format=logfmt)
    logging.addLevelName(99,'UBER')

    processor = Processor(args.es, args.index, args.size, args.timeout, args.flatten, args.query, args.max_partition_size_mb)
    processor.process()
